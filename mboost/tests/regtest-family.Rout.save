
R Under development (unstable) (2022-04-05 r82091) -- "Unsuffered Consequences"
Copyright (C) 2022 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> 
> library("mboost")
Loading required package: parallel
Loading required package: stabs
> 
> set.seed(290875)
> x <- rnorm(100)
> y <- rnorm(100)
> w <- drop(rmultinom(1, 100, rep(1 / 100, 100)))
> 
> G <- Gaussian()
> fm <- Family(ngradient = G@ngradient, risk = G@risk)
> 
> glmboost(y ~ x, family = G)

	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = y ~ x, family = G)


	 Squared Error (Regression) 

Loss function: (y - f)^2 
 

Number of boosting iterations: mstop = 100 
Step size:  0.1 
Offset:  0.04741503 

Coefficients: 
 (Intercept)            x 
-0.002576133  0.018571944 
attr(,"offset")
[1] 0.04741503

> 
> glmboost(y ~ x, family = fm)

	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = y ~ x, family = fm)


	 user-specified 

Loss function:  

Number of boosting iterations: mstop = 100 
Step size:  0.1 
Offset:  0.04741503 

Coefficients: 
 (Intercept)            x 
-0.002576133  0.018571944 
attr(,"offset")
[1] 0.04741503

> 
> mG <- glmboost(y ~ x, family = Gaussian())
> mH <- glmboost(y ~ x, family = Huber(10))
> all.equal(coef(mG), coef(mH))
[1] TRUE
> 
> 
> x <- rnorm(100)
> y <- rnbinom(length(x), size = 2, mu = exp(x * 2))
> mod <- glmboost(y ~ x, family = NBinomial(), center = FALSE)
> mod[1000]

	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = y ~ x, family = NBinomial(), center = FALSE)


	 Negative Negative-Binomial Likelihood 

Loss function:  

Number of boosting iterations: mstop = 1000 
Step size:  0.1 
Offset:  1.633154 

Coefficients: 
(Intercept)           x 
  -1.481151    1.912433 
attr(,"offset")
[1] 1.633154

> coef(mod)
(Intercept)           x 
  -1.481151    1.912433 
attr(,"offset")
[1] 1.633154
> nuisance(mod)
[1] 1.331289
> 
> ### QuantReg and ExpectReg
> gamboost(y ~ x, family = QuantReg())

	 Model-based Boosting

Call:
gamboost(formula = y ~ x, family = QuantReg())


	 Quantile Regression 

Loss function: tau * (y - f) * ((y - f) >= 0) - (1 - tau) * (y - f) * ((y -  
     f) < 0) 
 

Number of boosting iterations: mstop = 100 
Step size:  0.1 
Offset:  1 
Number of baselearners:  1 

> gamboost(y ~ x, family = ExpectReg())

	 Model-based Boosting

Call:
gamboost(formula = y ~ x, family = ExpectReg())


	 Expectile Regression 

Loss function: tau * (y - f)^2 * ((y - f) >= 0) + (1 - tau) * (y - f)^2 * ((y -  
     f) < 0) 
 

Number of boosting iterations: mstop = 100 
Step size:  0.1 
Offset:  5.12 
Number of baselearners:  1 

> 
> if (require("MASS")) {
+ 
+ summary(glm.nb(y ~ x))
+ 
+ y <- cut(x, breaks = c(-Inf, quantile(x, prob = c(0.25, 0.5, 0.75)), Inf), ordered = TRUE)
+ x <- rnorm(100)
+ polr(y ~ x)
+ mod <- glmboost(y ~ x, family = PropOdds(), center = FALSE)
+ nuisance(mod) - attr(coef(mod), "offset")
+ coef(mod)
+ 
+ }
Loading required package: MASS
         x 
0.09113743 
attr(,"offset")
[1] -0.06195115
> 
> ## Count models as before but with highly skewed data 
> ## (used to lead to an error as offset was not properly computed)
> x <- rnorm(100)
> y <- rnbinom(length(x), size = 2, mu = exp(x * 5))
> mod <- glmboost(y ~ x, family = NBinomial())
> ## check same for Hurdle model
> x <- x[y > 0]
> y <- y[y > 0]
> mod <- glmboost(y ~ x, family = Hurdle())
> 
> ### Weibull model
> 
> if (require("survival")) {
+ 
+ rextrval <- function(x) log( -log(1-x) )
+ sigma <- 0.5
+ u <- runif(100)
+ u.1 <- runif(100)
+ w <- rextrval(u)
+ w.1 <- rextrval(u.1)
+ 
+ x1 <- rnorm(100,sd=1)
+ x2 <- x1 + rnorm(100,sd=1)
+ x1.1 <- rnorm(100,sd=1)
+ x2.1 <- x1.1 + rnorm(100,sd=1)
+ X <- cbind(x1,x2)
+ X.1 <- cbind(x1.1,x2.1)
+ beta <- c(1,0.5)
+ survtime <- exp(X%*%beta + sigma*w)
+ censtime <- exp(X.1%*%beta + sigma*w.1)
+ event <- survtime < censtime
+ stime <- pmin(survtime,censtime)
+ 
+ model1 <- glmboost(Surv(stime,event)~x1+x2, family=Weibull(),
+     control = boost_control(mstop=100), center = FALSE)
+ coef(model1)
+ nuisance(model1)
+ model2 <- survreg(Surv(stime,event)~x1+x2)
+ coef(model2)
+ model2$scale
+ 
+ }
Loading required package: survival
[1] 0.4392436
> 
> 
> ### Log logistic model
> 
> if (require("survival")) {
+ 
+ sigma <- 0.5
+ w <- rlogis(100)
+ w.1 <- rlogis(100)
+ 
+ x1 <- rnorm(100,sd=1)
+ x2 <- x1 + rnorm(100,sd=1)
+ x1.1 <- rnorm(100,sd=1)
+ x2.1 <- x1.1 + rnorm(100,sd=1)
+ X <- cbind(x1,x2)
+ X.1 <- cbind(x1.1,x2.1)
+ beta <- c(1,0.5)
+ survtime <- exp(X%*%beta + sigma*w)
+ censtime <- exp(X.1%*%beta + sigma*w.1)
+ event <- survtime < censtime
+ stime <- pmin(survtime,censtime)
+ 
+ model1 <- glmboost(Surv(stime,event)~x1+x2, family=Loglog(),
+     control = boost_control(mstop=200), center = FALSE)
+ coef(model1)
+ nuisance(model1)
+ model2 <- survreg(Surv(stime,event)~x1+x2, dist="loglogistic")
+ coef(model2)
+ model2$scale
+ 
+ }
[1] 0.5179667
> 
> 
> ### Log normal model
> 
> if (require("survival")) {
+ 
+ sigma <- 0.5
+ w <- rnorm(100)
+ w.1 <- rnorm(100)
+ 
+ x1 <- rnorm(100,sd=1)
+ x2 <- x1 + rnorm(100,sd=1)
+ x1.1 <- rnorm(100,sd=1)
+ x2.1 <- x1.1 + rnorm(100,sd=1)
+ X <- cbind(x1,x2)
+ X.1 <- cbind(x1.1,x2.1)
+ beta <- c(1,0.5)
+ survtime <- exp(X%*%beta + sigma*w)
+ censtime <- exp(X.1%*%beta + sigma*w.1)
+ event <- survtime < censtime
+ stime <- pmin(survtime,censtime)
+ 
+ model1 <- glmboost(Surv(stime,event)~x1+x2, family=Lognormal(),
+     control = boost_control(mstop=200), center = FALSE)
+ coef(model1)
+ nuisance(model1)
+ model2 <- survreg(Surv(stime,event)~x1+x2, dist="lognormal")
+ coef(model2)
+ model2$scale
+ 
+ }
[1] 0.4021169
> 
> 
> ### AUC
> data("wpbc", package = "TH.data")
> wpbc[,colnames(wpbc) != "status"] <- scale(wpbc[,colnames(wpbc) != "status"])
> wpbc <- wpbc[complete.cases(wpbc), colnames(wpbc) != "time"]
> mAUC <- gamboost(status ~ ., data = wpbc, family = AUC())
> 1 - mAUC$risk()
  [1] 0.0000000 0.5779965 0.7027027 0.7151880 0.7303173 0.7426557 0.7488249
  [8] 0.7573443 0.7643948 0.7658637 0.7720329 0.7786428 0.7812867 0.7837838
 [15] 0.7899530 0.7917156 0.7934783 0.7965629 0.7993537 0.8019976 0.8049354
 [22] 0.8080200 0.8102233 0.8130141 0.8178613 0.8197709 0.8235899 0.8300529
 [29] 0.8332844 0.8344595 0.8398942 0.8418038 0.8445946 0.8479730 0.8478261
 [36] 0.8523796 0.8531140 0.8608989 0.8633960 0.8658931 0.8689777 0.8744125
 [43] 0.8776439 0.8852820 0.8889542 0.8936545 0.8977673 0.9055523 0.9079025
 [50] 0.9098120 0.9130435 0.9196533 0.9249412 0.9237662 0.9253819 0.9303760
 [57] 0.9353702 0.9409518 0.9418331 0.9469741 0.9450646 0.9480024 0.9509401
 [64] 0.9543184 0.9513807 0.9537309 0.9575499 0.9568155 0.9599001 0.9600470
 [71] 0.9610752 0.9616627 0.9618096 0.9622503 0.9644536 0.9654818 0.9650411
 [78] 0.9675382 0.9681257 0.9681257 0.9681257 0.9710635 0.9731199 0.9715041
 [85] 0.9726792 0.9729730 0.9751763 0.9759107 0.9767920 0.9763514 0.9767920
 [92] 0.9778202 0.9781140 0.9787015 0.9785546 0.9787015 0.9785546 0.9791422
 [99] 0.9788484 0.9792891 0.9795828
> 
> 
> # rank-based boosting
> 
> if (require("survival")) {
+ 
+ set.seed(1907)
+ n <- 100
+ beta <- c(3, 1.5, 0, 0, 2, 0, 0, 0)
+ p <- length(beta)
+ x <- matrix(rnorm(n*p), n, p)
+ yt <- x %*% beta + rnorm(n)
+ x <- cbind(1, x)
+ colnames(x) <- c("intercept", paste("x", 1:8, sep=""))
+ cens <- runif(n, 0, 6)
+ y <- exp(pmin(yt, cens))
+ del <- yt <= cens
+ ## check fitting and cvrisk
+ mod <- glmboost(x = x, y = Surv(y, del),
+                 control = boost_control(mstop = 500, nu = 0.1),
+                 center = TRUE,
+                 family = Gehan())
+ coef(mod)
+ plot(mod$risk())
+ cvr <- cvrisk(mod, folds = cv(model.weights(mod), "kfold"), papply=lapply)
+ plot(cvr)
+ ## check weighting:
+ wMat <- cv(rep(1, n), type = "kfold",B = 2)
+ modWeighted <- glmboost(x = x, y = Surv(y, del), weights = wMat[, 1],
+                         control = boost_control(mstop = 300, nu = 0.20),
+                         family = Gehan())
+ # same model with data set subseted:
+ modSubset <- glmboost(x = x[as.logical(wMat[, 1]),],
+                       y = Surv(y, del)[as.logical(wMat[, 1]),],
+                       control = boost_control(mstop = 300, nu = 0.20),
+                       family = Gehan())
+ ## <FIXME> there are still some minor discrepancies. Perhaps this is due to
+ ## different pre-processing? </FIXME>
+ round(coef(modWeighted, which = "") - coef(modSubset, which = ""), 3)
+ }
intercept        x1        x2        x3        x4        x5        x6        x7 
   -0.609     0.000    -0.004     0.001    -0.001    -0.003     0.000     0.000 
       x8 
   -0.004 
attr(,"offset")
[1] 5.867733
> 
> 
> ## Binomial
> y <- as.factor(sample(0:1, 100, replace = TRUE))
> x1 <- rnorm(100)
> x2 <- rnorm(100)
> 
> mod <- glmboost(y ~ x1 + x2, family = Binomial())
> mod[500]

	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = y ~ x1 + x2, family = Binomial())


	 Negative Binomial Likelihood (logit link) 

Loss function: { 
     f <- pmin(abs(f), 36) * sign(f) 
     p <- exp(f)/(exp(f) + exp(-f)) 
     y <- (y + 1)/2 
     -y * log(p) - (1 - y) * log(1 - p) 
 } 
 

Number of boosting iterations: mstop = 500 
Step size:  0.1 
Offset:  -0.06007216 

Coefficients: 

NOTE: Coefficients from a Binomial model are half the size of coefficients
 from a model fitted via glm(... , family = 'binomial').
See Warning section in ?coef.mboost

 (Intercept)           x1           x2 
-0.007930788 -0.010047474  0.086191754 
attr(,"offset")
[1] -0.06007216

> 2 * coef(mod, off2int = TRUE)

NOTE: Coefficients from a Binomial model are half the size of coefficients
 from a model fitted via glm(... , family = 'binomial').
See Warning section in ?coef.mboost

(Intercept)          x1          x2 
-0.13600589 -0.02009495  0.17238351 
> 
> glmMod <- glm(y ~ x1 + x2, family = 'binomial')
> coef(glmMod)
(Intercept)          x1          x2 
-0.13600589 -0.02009495  0.17238351 
> stopifnot(all(abs((coef(glmMod) - coef(mod, off2int = TRUE) * 2)) < sqrt(.Machine$double.eps)))

NOTE: Coefficients from a Binomial model are half the size of coefficients
 from a model fitted via glm(... , family = 'binomial').
See Warning section in ?coef.mboost

> 
> ## C-index boosting
> if (require("survival")) {
+   
+   sigma <- 0.5
+   w <- rnorm(100)
+   w.1 <- rnorm(100)
+   
+   x1 <- rnorm(100,sd=1)
+   x2 <- x1 + rnorm(100,sd=1)
+   x1.1 <- rnorm(100,sd=1)
+   x2.1 <- x1.1 + rnorm(100,sd=1)
+   X <- cbind(x1,x2)
+   X.1 <- cbind(x1.1,x2.1)
+   beta <- c(1,0.5)
+   survtime <- exp(X%*%beta + sigma*w)
+   censtime <- exp(X.1%*%beta + sigma*w.1)
+   event <- survtime < censtime
+   stime <- pmin(survtime,censtime)
+   dat <- data.frame(time = stime, event = event, x1 = x1, x2 = x2)
+   
+   # compute ipcweights outside the family
+   ipcw <- IPCweights(x = Surv(dat$time, dat$event))
+   model1 <- glmboost(Surv(time,event)~x1+x2, family=Cindex(ipcw = ipcw),
+                      control = boost_control(mstop=50), data = dat)
+   
+   
+   # compute ipcweights inside
+   model2 <- glmboost(Surv(time,event)~x1+x2, family=Cindex(ipcw = 1),
+                      control = boost_control(mstop=50), data = dat)
+   rbind(coef(model1, off2int = TRUE, which = ""), 
+         coef(model2, off2int = TRUE, which = ""))
+   
+   stopifnot(identical(coef(model1), coef(model2)))
+   
+   # change sigma
+   model1 <- glmboost(Surv(time,event)~x1+x2, family=Cindex(sigma = 0.01),
+                      control = boost_control(mstop=20), data = dat)
+   model2 <- glmboost(Surv(time,event)~x1+x2, family=Cindex(sigma = 0.2),
+                      control = boost_control(mstop=20), data = dat)
+   rbind(coef(model1, off2int = TRUE, which = ""), 
+         coef(model2, off2int = TRUE, which = ""))
+   stopifnot(!identical(coef(model1), coef(model2)))
+   
+ }
> 
> ## check Binomial(type = "glm")
> glmModboost <- glmboost(y ~ x1 + x2, family = Binomial(type = "glm"))
> glmModboost[1000] 

	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = y ~ x1 + x2, family = Binomial(type = "glm"))


	 Binomial Distribution (similar to glm) 

Loss function: { 
     ntrials <- rowSums(y) 
     y <- y[, 1] 
     p <- link$linkinv(f) 
     -dbinom(x = y, size = ntrials, prob = p, log = log) 
 } 
 

Number of boosting iterations: mstop = 1000 
Step size:  0.1 
Offset:  -0.06001801 

Coefficients: 
(Intercept)          x1          x2 
-0.09562402 -0.29960765  0.21829889 
attr(,"offset")
[1] -0.06001801

> round(rbind(coef(glmMod), coef(glmModboost, off2int =TRUE), 2*coef(mod, off2int = TRUE)),3)

NOTE: Coefficients from a Binomial model are half the size of coefficients
 from a model fitted via glm(... , family = 'binomial').
See Warning section in ?coef.mboost

     (Intercept)    x1    x2
[1,]      -0.136 -0.02 0.172
[2,]      -0.156 -0.30 0.218
[3,]      -0.136 -0.02 0.172
> ## use different link
> glmMod <- glm(y ~ x1 + x2, family = binomial(link = "probit"))
> coef(glmMod)
(Intercept)          x1          x2 
 -0.1018936  -0.2012023   0.1448793 
> glmModboost <- glmboost(y ~ x1 + x2, family = Binomial(type = "glm", link = "probit"))
> glmModboost[500]

	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = y ~ x1 + x2, family = Binomial(type = "glm",     link = "probit"))


	 Binomial Distribution (similar to glm) 

Loss function: { 
     ntrials <- rowSums(y) 
     y <- y[, 1] 
     p <- link$linkinv(f) 
     -dbinom(x = y, size = ntrials, prob = p, log = log) 
 } 
 

Number of boosting iterations: mstop = 500 
Step size:  0.1 
Offset:  -0.03760829 

Coefficients: 
(Intercept)          x1          x2 
-0.06246989 -0.19603232  0.14148039 
attr(,"offset")
[1] -0.03760829

> round(rbind(coef(glmMod), coef(glmModboost, off2int =TRUE)),3)
     (Intercept)     x1    x2
[1,]      -0.102 -0.201 0.145
[2,]      -0.100 -0.196 0.141
> ## use matrix of successes and failures
> y <- matrix(ncol = 2, nrow = length(x1), data = rpois(lambda = 30, n =2*length(x1)) )
> glmMod <- glm(y ~ x1 + x2, family = binomial())
> coef(glmMod)
(Intercept)          x1          x2 
 0.01105974  0.00994127 -0.03606502 
> glmModboost <- glmboost(y ~ x1 + x2, family = Binomial(type = "glm"))
> glmModboost[500]

	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = y ~ x1 + x2, family = Binomial(type = "glm"))


	 Binomial Distribution (similar to glm) 

Loss function: { 
     ntrials <- rowSums(y) 
     y <- y[, 1] 
     p <- link$linkinv(f) 
     -dbinom(x = y, size = ntrials, prob = p, log = log) 
 } 
 

Number of boosting iterations: mstop = 500 
Step size:  0.1 
Offset:  0.006259969 

Coefficients: 
 (Intercept)           x1           x2 
 0.004799774  0.009941270 -0.036065023 
attr(,"offset")
[1] 0.006259969

> round(rbind(coef(glmMod), coef(glmModboost, off2int =TRUE)),3)
     (Intercept)   x1     x2
[1,]       0.011 0.01 -0.036
[2,]       0.011 0.01 -0.036
> ## use binary vector
> y <- rbinom(prob = plogis(x1 + x2), size = 1, n = length(x1))
> glmMod <- glm(y ~ x1 + x2, family = binomial())
> coef(glmMod)
(Intercept)          x1          x2 
  0.1683506   1.5157657   0.7426741 
> glmModboost <- glmboost(y ~ x1 + x2, family = Binomial(type = "glm"),
+                         control = boost_control(nu = 0.2))
> glmModboost[1000]

	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = y ~ x1 + x2, family = Binomial(type = "glm"),     control = boost_control(nu = 0.2))


	 Binomial Distribution (similar to glm) 

Loss function: { 
     ntrials <- rowSums(y) 
     y <- y[, 1] 
     p <- link$linkinv(f) 
     -dbinom(x = y, size = ntrials, prob = p, log = log) 
 } 
 

Number of boosting iterations: mstop = 1000 
Step size:  0.2 
Offset:  0.1000835 

Coefficients: 
(Intercept)          x1          x2 
 0.06656884  1.51242634  0.74061787 
attr(,"offset")
[1] 0.1000835

> round(rbind(coef(glmMod), coef(glmModboost, off2int =TRUE)),2)
     (Intercept)   x1   x2
[1,]        0.17 1.52 0.74
[2,]        0.17 1.51 0.74
> 
> 
> ## 
> ## Binomial with other links
> ## and interface type = "adaboost" or "glm"
> set.seed(123)
> y <- as.factor(sample(0:1, 100, replace = TRUE))
> x1 <- rnorm(100)
> x2 <- rnorm(100)
> 
> mod <- glmboost(y ~ x1 + x2, family = Binomial(type = "adaboost", link = "cauchit"))
> mod[500]

	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = y ~ x1 + x2, family = Binomial(type = "adaboost",     link = "cauchit"))


	 Negative Binomial Likelihood -- cauchit link 

Loss function: { 
     p <- link$linkinv(f) 
     y <- (y + 1)/2 
     -y * log(p) - (1 - y) * log(1 - p) 
 } 
 

Number of boosting iterations: mstop = 500 
Step size:  0.1 
Offset:  -0.2235265 

Coefficients: 

NOTE: Coefficients from a Binomial model are half the size of coefficients
 from a model fitted via glm(... , family = 'binomial').
See Warning section in ?coef.mboost

 (Intercept)           x1           x2 
-0.007410285 -0.017629960 -0.197800409 
attr(,"offset")
[1] -0.2235265

> mod2 <- glmboost(y ~ x1 + x2, family = Binomial(type = "glm", link = "cauchit"))
> mod2[500]

	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = y ~ x1 + x2, family = Binomial(type = "glm",     link = "cauchit"))


	 Binomial Distribution (similar to glm) 

Loss function: { 
     ntrials <- rowSums(y) 
     y <- y[, 1] 
     p <- link$linkinv(f) 
     -dbinom(x = y, size = ntrials, prob = p, log = log) 
 } 
 

Number of boosting iterations: mstop = 500 
Step size:  0.1 
Offset:  -0.110401 

Coefficients: 
(Intercept)          x1          x2 
-0.12023054 -0.01733872 -0.19738975 
attr(,"offset")
[1] -0.110401

> glmMod <- glm(y ~ x1 + x2 , family = binomial(link = "cauchit"))
> stopifnot(all(round(coef(glmMod),2) == round(coef(mod, off2int =TRUE),2)))

NOTE: Coefficients from a Binomial model are half the size of coefficients
 from a model fitted via glm(... , family = 'binomial').
See Warning section in ?coef.mboost

> rbind(coef(glmMod), coef(mod, off2int = TRUE), coef(mod2, off2int = TRUE))

NOTE: Coefficients from a Binomial model are half the size of coefficients
 from a model fitted via glm(... , family = 'binomial').
See Warning section in ?coef.mboost

     (Intercept)          x1         x2
[1,]  -0.2312005 -0.01788017 -0.1981612
[2,]  -0.2309368 -0.01762996 -0.1978004
[3,]  -0.2306316 -0.01733872 -0.1973898
> 
> mod <- glmboost(y ~ x1 + x2, family = Binomial(type = "adaboost", link = "probit"))
> mod[500]

	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = y ~ x1 + x2, family = Binomial(type = "adaboost",     link = "probit"))


	 Negative Binomial Likelihood -- probit link 

Loss function: { 
     p <- link$linkinv(f) 
     y <- (y + 1)/2 
     -y * log(p) - (1 - y) * log(1 - p) 
 } 
 

Number of boosting iterations: mstop = 500 
Step size:  0.1 
Offset:  -0.1763742 

Coefficients: 

NOTE: Coefficients from a Binomial model are half the size of coefficients
 from a model fitted via glm(... , family = 'binomial').
See Warning section in ?coef.mboost

  (Intercept)            x1            x2 
-6.006544e-05 -1.129000e-02 -1.404508e-01 
attr(,"offset")
[1] -0.1763742

> glmMod <- glm(y ~ x1 + x2 , family = binomial(link = "probit"))
> stopifnot(all(round(coef(glmMod),2) == round(coef(mod, off2int =TRUE),2)))

NOTE: Coefficients from a Binomial model are half the size of coefficients
 from a model fitted via glm(... , family = 'binomial').
See Warning section in ?coef.mboost

> mod2 <- glmboost(y ~ x1 + x2, family = Binomial(type = "glm", link = "probit"))
> mod2[500]

	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = y ~ x1 + x2, family = Binomial(type = "glm",     link = "probit"))


	 Binomial Distribution (similar to glm) 

Loss function: { 
     ntrials <- rowSums(y) 
     y <- y[, 1] 
     p <- link$linkinv(f) 
     -dbinom(x = y, size = ntrials, prob = p, log = log) 
 } 
 

Number of boosting iterations: mstop = 500 
Step size:  0.1 
Offset:  -0.08784484 

Coefficients: 
(Intercept)          x1          x2 
-0.08858786 -0.01128836 -0.14044916 
attr(,"offset")
[1] -0.08784484

> rbind(coef(glmMod), coef(mod, off2int = TRUE), coef(mod2, off2int = TRUE))

NOTE: Coefficients from a Binomial model are half the size of coefficients
 from a model fitted via glm(... , family = 'binomial').
See Warning section in ?coef.mboost

     (Intercept)          x1         x2
[1,]  -0.1764348 -0.01129052 -0.1404515
[2,]  -0.1764342 -0.01129000 -0.1404508
[3,]  -0.1764327 -0.01128836 -0.1404492
> 
> 
> mod <- glmboost(y ~ x1 + x2, family = Binomial(type = "adaboost", link = "log"))
> mod[500]

	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = y ~ x1 + x2, family = Binomial(type = "adaboost",     link = "log"))


	 Negative Binomial Likelihood -- log link 

Loss function: { 
     p <- link$linkinv(f) 
     y <- (y + 1)/2 
     -y * log(p) - (1 - y) * log(1 - p) 
 } 
 

Number of boosting iterations: mstop = 500 
Step size:  0.1 
Offset:  -0.8439701 

Coefficients: 

NOTE: Coefficients from a Binomial model are half the size of coefficients
 from a model fitted via glm(... , family = 'binomial').
See Warning section in ?coef.mboost

 (Intercept)           x1           x2 
-0.005874508 -0.003759397 -0.141144724 
attr(,"offset")
[1] -0.8439701

> glmMod <- glm(y ~ x1 + x2 , family = binomial(link = "log"))
> stopifnot(all(round(coef(glmMod),2) == round(coef(mod, off2int =TRUE),2)))

NOTE: Coefficients from a Binomial model are half the size of coefficients
 from a model fitted via glm(... , family = 'binomial').
See Warning section in ?coef.mboost

> mod2 <- glmboost(y ~ x1 + x2, family = Binomial(type = "glm", link = "log"))
> mod2[500]

	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = y ~ x1 + x2, family = Binomial(type = "glm",     link = "log"))


	 Binomial Distribution (similar to glm) 

Loss function: { 
     ntrials <- rowSums(y) 
     y <- y[, 1] 
     p <- link$linkinv(f) 
     -dbinom(x = y, size = ntrials, prob = p, log = log) 
 } 
 

Number of boosting iterations: mstop = 500 
Step size:  0.1 
Offset:  -0.7657179 

Coefficients: 
 (Intercept)           x1           x2 
-0.084126052 -0.003758924 -0.141143977 
attr(,"offset")
[1] -0.7657179

> rbind(coef(glmMod), coef(mod, off2int = TRUE), coef(mod2, off2int = TRUE))

NOTE: Coefficients from a Binomial model are half the size of coefficients
 from a model fitted via glm(... , family = 'binomial').
See Warning section in ?coef.mboost

     (Intercept)           x1         x2
[1,]  -0.8498452 -0.003762303 -0.1411426
[2,]  -0.8498446 -0.003759397 -0.1411447
[3,]  -0.8498439 -0.003758924 -0.1411440
> 
> 
> proc.time()
   user  system elapsed 
  9.246   0.120   9.368 
